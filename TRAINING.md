# PPO Training System for Air Hockey - Complete Overview

## ðŸ“‹ System Architecture

Your PPO training system consists of 5 main components:

### 1. **Training Script** (`train_ppo.py`)
- **Main PPO Algorithm**: Implements Proximal Policy Optimization
- **Environment Wrapper**: Gym-like interface around your `BaseEngine`
- **Actor-Critic Network**: Policy and value function networks
- **Training Loop**: Rollout collection, advantage estimation, policy updates

### 2. **Configuration** (`config/ppo_config.yaml`)
- All hyperparameters in one place
- Easy to tune without code changes
- Includes training, environment, and logging settings

### 3. **Testing Suite** (`test_policy.py`)
- Evaluate trained policies
- Compare multiple checkpoints
- Visualize agent behavior
- Generate performance metrics

### 4. **ROS Converter** (`convert_ppo_to_ros.py`)
- Converts PPO checkpoints to ROS-compatible format
- Creates pickle files for agent nodes
- Handles weight extraction and mapping

### 5. **Quick Start Script** (`quickstart.sh`)
- Interactive menu for all operations
- Dependency checking
- Automated workflow management

## ðŸš€ Quick Start Guide

### Step 1: Setup

```bash
# Make quick start script executable
chmod +x quickstart.sh

# Run it
./quickstart.sh
```

The script will:
- âœ… Check all dependencies
- âœ… Create necessary directories
- âœ… Generate default configuration
- âœ… Show interactive menu

### Step 2: Training

**Option A: Using Quick Start Script**
```bash
./quickstart.sh
# Select: 1) Start training from scratch
```

**Option B: Direct Command**
```bash
python train_ppo.py --config config/ppo_config.yaml
```

### Step 3: Monitor Progress

```bash
# View training logs
tail -f logs/training.log

# Or use quick start menu
./quickstart.sh
# Select: 6) Visualize training logs
```

### Step 4: Test Your Policy

```bash
# Interactive testing
./quickstart.sh
# Select: 3) Test trained policy

# Or direct command
python test_policy.py \
    --checkpoint checkpoints/ppo_checkpoint_1000.pt \
    --episodes 50 \
    --visualize
```

### Step 5: Deploy to ROS

```bash
# Convert checkpoint
./quickstart.sh
# Select: 5) Convert checkpoint to ROS format

# Or direct command
python convert_ppo_to_ros.py \
    --checkpoint checkpoints/ppo_checkpoint_1000.pt \
    --output policies/trained_ppo \
    --num-agents 2
```

Then update your launch file:
```python
# In single_game.launch.py
team_a = "trained_ppo"
# Make sure ppo_checkpoint_path is set
```

## ðŸ“Š Understanding the Training Process

### 1. Observation Space
Each agent observes:
- **Self position** (2D): Normalized x, y coordinates
- **Puck state** (4D): Position (x, y) and velocity (vx, vy)
- **Teammates** (5D each): Relative positions and puck distances
- **Opponents** (5D each): Relative positions and puck distances

Total: `2 + 4 + (num_teammates Ã— 5) + (num_opponents Ã— 5)` dimensions

### 2. Action Space
- **Continuous**: Actions in [-1, 1]Â² from neural network
- **Discretized**: Converted to {-1, 0, 1}Â² for simulation
- **Threshold**: 0.33 deadzone for more stable control

### 3. Reward Function
```python
# Goal rewards (sparse)
+10.0  for scoring a goal
-10.0  for conceding a goal

# Shaping rewards (dense)
-0.001 Ã— distance_to_puck    # Encourage defensive positioning
-0.01  if too_far_from_side  # Stay in defensive half
-0.0001 Ã— |action|           # Energy efficiency
```

### 4. PPO Update Process

```
For each rollout:
  1. Collect n_steps of experience
  2. Compute advantages using GAE
  3. For n_epochs:
       For each mini-batch:
         - Compute policy loss (PPO clip)
         - Compute value loss (MSE)
         - Add entropy bonus
         - Update networks
  4. Log metrics
  5. Save checkpoint
```

## ðŸŽ¯ Hyperparameter Tuning Guide

### Common Issues & Solutions

#### Issue: Training is unstable
**Solutions:**
```yaml
# Reduce learning rate
learning_rate: 0.0001  # from 0.0003

# Reduce clip range
clip_range: 0.1  # from 0.2

# Increase batch size
batch_size: 128  # from 64
```

#### Issue: Agents don't learn
**Solutions:**
```yaml
# Increase exploration
ent_coef: 0.05  # from 0.01

# Adjust reward scale
# Edit _compute_rewards() in train_ppo.py
reward *= 0.1  # Scale down if rewards are too large

# Increase training time
total_timesteps: 5000000  # from 1000000
```

#### Issue: Too slow
**Solutions:**
```yaml
# More steps per update
n_steps: 4096  # from 2048

# Fewer optimization epochs
n_epochs: 5  # from 10

# Smaller network
hidden_dim: 64  # from 128
```

## ðŸ“ˆ Performance Benchmarks

### Expected Learning Curve

| Timesteps | Win Rate vs Random | Win Rate vs Simple | Avg Score |
|-----------|-------------------|-------------------|-----------|
| 100K      | 30-40%            | 10-20%            | 1.2       |
| 500K      | 60-70%            | 30-40%            | 2.5       |
| 1M        | 80-90%            | 50-60%            | 3.8       |
| 2M+       | 95%+              | 70-80%            | 4.5       |

### Training Time Estimates

- **CPU (Intel i7)**: ~10-15 hours for 1M timesteps
- **GPU (RTX 3080)**: ~3-5 hours for 1M timesteps
- **GPU (RTX 4090)**: ~1-2 hours for 1M timesteps

## ðŸ”§ Advanced Features

### 1. Curriculum Learning

Gradually increase difficulty:

```python
# In train_ppo.py, modify rollout collection
def get_opponent_skill(self, rollout_num):
    if rollout_num < 100:
        return 'random'
    elif rollout_num < 500:
        return 'simple'
    else:
        return 'advanced'
```

### 2. Multi-Agent Coordination

Train with communication:

```python
# Add to observation
teammate_velocities = [...]  # Velocity of teammates
teammate_actions = [...]     # Last actions of teammates
```

### 3. Opponent Modeling

Learn opponent patterns:

```python
# Add opponent prediction network
class OpponentPredictor(nn.Module):
    def forward(self, history):
        return predicted_action
```

### 4. Self-Play Training

Train against previous versions:

```python
# Maintain pool of past policies
policy_pool = []
every_N_checkpoints:
    policy_pool.append(current_policy.copy())
    
# Sample opponent from pool
opponent = random.choice(policy_pool)
```

## ðŸ“ File Structure

```
project/
â”œâ”€â”€ train_ppo.py              # Main training script
â”œâ”€â”€ test_policy.py            # Testing & evaluation
â”œâ”€â”€ convert_ppo_to_ros.py     # ROS converter
â”œâ”€â”€ quickstart.sh             # Interactive menu
â”œâ”€â”€ config/
â”‚   â””â”€â”€ ppo_config.yaml       # Hyperparameters
â”œâ”€â”€ checkpoints/              # Saved models
â”‚   â”œâ”€â”€ ppo_checkpoint_100.pt
â”‚   â”œâ”€â”€ ppo_checkpoint_200.pt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training.log          # Training logs
â”œâ”€â”€ results/                  # Test results
â”‚   â”œâ”€â”€ test_results_*.json
â”‚   â””â”€â”€ comparison_*.json
â””â”€â”€ policies/
    â””â”€â”€ trained_ppo/          # ROS-compatible policies
        â”œâ”€â”€ ppo_agent_0.pkl
        â””â”€â”€ ppo_agent_1.pkl
```

## ðŸ› Troubleshooting

### Problem: "CUDA out of memory"
```yaml
# Solution 1: Reduce batch size
batch_size: 32

# Solution 2: Use CPU
device: "cpu"

# Solution 3: Reduce network size
hidden_dim: 64
```

### Problem: "Agents stuck in corners"
```python
# Solution: Add penalty for wall proximity
def _compute_rewards(...):
    # Add to rewards
    wall_dist = min(
        agent_x, width - agent_x,
        agent_y, height - agent_y
    )
    if wall_dist < 50:
        rewards[i] -= 0.05
```

### Problem: "Training diverges"
```yaml
# Solution 1: Reduce learning rate
learning_rate: 0.0001

# Solution 2: Increase gradient clipping
max_grad_norm: 0.1

# Solution 3: Add value function clipping
clip_range_vf: 0.2
```

## ðŸŽ“ Learning Resources

### PPO Papers
- [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
- [Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)

### Tutorials
- [OpenAI Spinning Up](https://spinningup.openai.com/)
- [Stable Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [CleanRL](https://github.com/vwxyzjn/cleanrl)

### Multi-Agent RL
- [QMIX](https://arxiv.org/abs/1803.11485)
- [MAPPO](https://arxiv.org/abs/2103.01955)

## ðŸ“ž Support

For issues or questions:
1. Check logs: `logs/training.log`
2. Run diagnostics: `./quickstart.sh` â†’ Option 6
3. Test environment: `python test_policy.py --episodes 1 --visualize`
4. Validate config: Check `config/ppo_config.yaml` syntax

## ðŸš§ Future Enhancements

Potential additions to the training system:

1. **Parallel Environments**: Multiple simultaneous games
2. **Prioritized Experience Replay**: Focus on important transitions
3. **Intrinsic Motivation**: Curiosity-driven exploration
4. **Hierarchical RL**: High-level strategies + low-level control
5. **Transfer Learning**: Pre-train on simpler tasks
6. **Attention Mechanisms**: Better multi-agent coordination

## âœ… Checklist for First Training Run

- [ ] Dependencies installed (`./quickstart.sh` checks this)
- [ ] Config file created (`config/ppo_config.yaml`)
- [ ] Directories created (`checkpoints/`, `logs/`, `results/`)
- [ ] GPU available (check with `nvidia-smi`) or set `device: "cpu"`
- [ ] Enough disk space (~1GB for checkpoints)
- [ ] Training script runs without errors
- [ ] Can visualize first episode
- [ ] Logs are being written
- [ ] Checkpoints are being saved

Good luck with your training! ðŸš€